% !TEX root = ecoop14.tex
\section{Syntax}
\label{s:approach}

\subsection{Concrete Syntax}
We will now describe the concrete syntax of Wyvern declaratively, using the same layout-sensitive formalism that we have introduced for TSL grammars, developed recently by Adams \cite{Adams:2013:PPI:2429069.2429129}. Such a formalism is useful because it allows us to implement  layout-sensitive syntax, like that we've been describing, without relying on context-sensitive lexers or parsers. Most existing layout-sensitive languages (e.g. Python and Haskell) use hand-rolled context-sensitive lexers or parsers (keeping track of, for example, the indentation level using special \li{INDENT} and \li{DEDENT} tokens), but these are more problematic because they cannot be used to generate editor modes, syntax highlighters and other tools automatically. In particular, we will show how the forward references we have described can be correctly encoded without requiring a context-sensitive parser or lexer using this formalism. It is also useful that the TSL for \li{Parser}, above, uses the same parser technology as the host language, so that it can be used to generate quasiquotes.

Wyvern's concrete syntax, with a few minor omissions for concision, is shown in Figure~\ref{f-grammar}. We first review Adams' formalism in some additional detail, then describe some key features of this syntax.

\subsection{Background: Adams' Formalism}
For each terminal and non-terminal in a rule, Adams proposed associating with them a relational operator, such as =, > and $\geq$ to specify the indentation at which those terms need to be with respect to the non-terminal on the left-hand side of the rule. The indentation level of a term can be identified as the column at which the left-most character of that term appears (not simply the first character, in the case of terms that span multiple lines). The meaning of the comparison operators is akin to their mathematical meaning: = means that the term on the right-hand side has to be at exactly the same indentation as the term on the left-hand side; >  means that the term on the right-hand side has to be indented strictly further to the right than the term on the left-hand side; $\geq$ is like >, except the term on the right could also be at the same indentation level as the term on the left-hand side. For example, the production rule of the form \lstinline{A $\rightarrow$ B$^=$ C$^\geq$ D$^>$} approximately reads as: ``Term \lstinline{B} must be at the same indentation level as term \lstinline{A}, term \lstinline{C} may be at the same or a greater indentation level as term \lstinline{A}, and term \lstinline{D} must be at an indentation level greater than term \lstinline{A}'s.'' In particular, if \li{D} contains a \lstinline{NEWLINE} character, the next line must be indented past the position of the left-most character of \lstinline{A} (typically constructed so that it must appear at the beginning of a line). There are no constraints relating \lstinline{D} to \lstinline{B} or \lstinline{C} other than the standard sequencing constraint: the first character of \lstinline{D} must be to further in the file than the others. Using Adam's formalism, the grammars of real-world languages like Python and Haskell can be written declaratively. This formalism can be integrated into LR and LALR parser generators.

\input{grammar}

\subsection{Programs}

\begin{figure}[t]
\begin{minipage}{.57\textwidth}
\begin{lstlisting}
objtype T
  val y : HTML
let page : HTML->HTML = fn x : HTML => ~
  :html
    :body
      {x}
page(case(5 : Nat))
  Z(_) => (new : T).y
    val y : HTML = ~
      :h1 Zero!
  S(x) => ~
    :h1 Successor!
\end{lstlisting}
\end{minipage}%
\begin{minipage}{.47\textwidth}
 \centering
\[
\begin{array}{l}
\keyw{objtype} \ T\ \{ \\
  ~~~\keyw{val}\ y : HTML, \\
  ~~~\keyw{metadata} = (\keyw{new}\ \{\}) : Unit\ \}; \\
  (\boldsymbol\lambda page : HTML \rightarrow HTML\ .\\
  ~page(\keyw{case}(\lfloor 5 \rfloor : Nat)\ \{ \\
  ~~~~Z(\_) \Rightarrow ((\keyw{new}\ \{ \\
    ~~~~~~~\keyw{val}\ y : HTML = \lfloor :h1\ Zero! \rfloor\}) : T).y\ | \\
   ~~~~S(x) \Rightarrow \lfloor :h1\ Successor! \rfloor\})) \\
  (\boldsymbol\lambda x : HTML\ .\ \lfloor :html \\
    ~~~~~~~~~~~:body \\
      ~~~~~~~~~~~~~~\{x\} \rfloor)
\end{array}
\]
\end{minipage}
\caption{An example Wyvern program demonstrating forward references}
\label{fig:fwd-ref}
\end{figure}
An example Wyvern program showing several unique syntactic features of TSL Wyvern is shown in Fig. \ref{f-frefs}. The top level of a program (the \lstinline{p} non-terminal) consists of a series of type declarations -- object types using \lstinline{objtype} or case types using \lstinline{casetype} -- followed by an expression, \lstinline{e}. Each type declaration contains associated declarations -- signatures for fields and methods in  \lstinline{objdecls} and case declarations in \lstinline{casedecls} (not shown on the figure). Each also can also include a metadata declaration. Metadata is simply an expression associated with the type, used to store TSL logic (and in future work, other logic). Sequences of top-level declarations use the form \lstinline{p$^=$} to signify that all the succeeding \lstinline{p} terms must begin at the same indentation.\todo{fig ref}
\subsection{Forward Referenced Blocks}
Wyvern makes extensive use of forward referenced blocks to make its syntax clean. In particular, layout-delimited TSLs, the general-purpose introductory form for object types and the elimination form for case types and product types all use forward referenced blocks. Fig. \ref{f-frefs} shows all of these in use (assuming suitable definitions of casetypes \li{Nat} and \li{HTML}, not included). In the grammar, note particularly the rules for \li{let} and that inline literals, even those containing nested expressions with forward references, can be treated as expressions not containing forward references -- \emph{in the initial phase of parsing, before typechecking commences, all literal forms are left unparsed}.

\subsection{Abstract Syntax}
The concrete syntax of a Wyvern program, \li{p}, is parsed to produce a program in the abstract syntax, $\rho$, shown on the left side of Fig. \ref{fig:core2-syntax}. Forward references are internalized. In particular, note that all literal forms are unified into the abstract literal form $\lfloor body \rfloor$, including the layout-delimited form and number literals. The abstract syntax contains a form, $\keyw{fromTS}[\Gamma](e,e)$, that has no analog in the concrete syntax. This will be used internally to ensure hygiene, as we will discuss in the next section.

\begin{figure}[t]
\centering
\[
\begin{array}[t]{lll} 
\rho & \bnfdef & \keyw{objtype}~ t~ \{ \omega, \keyw{metadata}=e \}; \rho \\
     & \bnfalt & \keyw{casetype}~ t~ \{ \chi, \keyw{metadata}=e \}; \rho\\
     & \bnfalt & e
     \\[1ex]
e    & \bnfdef & x \\
     & \bnfalt & \boldsymbol\lambda x{:}\tau . e \\ %
     & \bnfalt & e(e) \\
     & \bnfalt & (e, e) \\
     & \bnfalt & \keyw{case}(e) \{(x, y) \Rightarrow e\}\\
     & \bnfalt & t.C(e) \\
     & \bnfalt & \keyw{case}(e)~\{ c \} \\
     & \bnfalt & \keyw{new}~ \{ d \}\\
     & \bnfalt & e.x \\
     & \bnfalt & e : \tau\\
     & \bnfalt & \keyw{valAST}(e) \\
     & \bnfalt & t.\keyw{metadata}\\
     & \bnfalt & \lfloor literal \rfloor \\
     & \bnfalt & \keyw{fromTS}[\Gamma](e, e)
\\[1ex]	
c    & \bnfdef & C(x) \Rightarrow e\\
     & \bnfalt & c \bnfalt c
	 \\[1ex]
d   & \bnfdef & \varepsilon \\
     & \bnfalt & \keyw{val}~ f:\tau = e;~d \\
     & \bnfalt & \keyw{def}~ m:\tau = e;~d
\\[1ex] 
\end{array}
\begin{array}[t]{lll}
~~~
\end{array}
\begin{array}[t]{lll}


\hat\rho & \bnfdef & \keyw{objtype}~ t~ \{ \omega, \keyw{metadata}=\hat e \}; \hat\rho \\
     & \bnfalt & \keyw{casetype}~ t~ \{ \chi, \keyw{metadata}=\hat e \}; \hat\rho\\
     & \bnfalt & \hat e
     \\[1ex]
\hat{e}    & \bnfdef & x \\
     & \bnfalt & \boldsymbol\lambda x{:}\tau . \hat{e} \\ %
     & \bnfalt & \hat{e}(\hat{e}) \\
     & \bnfalt & \cdots \\
     & \bnfalt & t.\keyw{metadata} 
\\[1ex]
\hat c    & \bnfdef & ...
	 \\[1ex]
\hat d   & \bnfdef & ... 
\\[1ex] 
\chi & \bnfdef & C~\keyw{of}~\tau\\
     & \bnfalt & \chi \bnfalt \chi 
\\[1ex]
\omega &\bnfdef & \varepsilon \\  
         & \bnfalt & \keyw{val}~ f:\tau;~\omega\\
         & \bnfalt & \keyw{def}~ m:\tau;~\omega 
\\[1ex]
\tau & \bnfdef & t\\
     & \bnfalt & \tau \rightarrow \tau \\
     & \bnfalt & \tau \times \tau 
\\[1ex]
\Gamma & \bnfdef & \emptyset \bnfalt \Gamma, x:\tau
\\[1ex]
\Delta & \bnfdef & \emptyset \bnfalt \Delta, t:\{\chi, \hat e:\tau\} \bnfalt \Delta, t:\{\omega, \hat e:\tau\}
\\[1ex]

\end{array}
\]
\caption{Abstract Syntax}
\label{fig:core2-syntax}
\end{figure}

\section{Bidirectional Typechecking and Literal Rewriting}
We will now specify a type system for the abstract syntax in Fig. \ref{fig:core2-syntax}. Conventional type systems are specified using a typechecking judgement like $\Delta; \Gamma \vdash e : \tau$, where the variable context, $\Gamma$, tracks the types of variables, and the type context, $\Delta$, tracks types and their signatures. However, this conventional formulation does not separately consider how, when deriving this judgement, it will be considered algorithmically -- will a type be provided, so that we simply need to check $e$ against it, or do we need to synthesize a type for $e$? For our system, this distinction is crucial: a generic literal can only be used in the first situation. 

\emph{Bidirectional type systems}, as presented by Lovas and Pfenning \cite{Lovas08abidirectional}, make this distinction clear by specifying the type system instead using two simultaneously defined judgements: one for expressions that can \emph{synthesize} a type based on the surrounding context (e.g. variables and elimination forms), and another for expressions for which we know what type to \emph{check} or \emph{analyze} the term against (e.g. generic literals and some introductory forms). 
Our work builds upon this work, making the following core additions: the tye context $\Delta$ now tracks the metadata in addition to type signatures, and as we typecheck, we need to also perform literal rewriting by calling the parser associated with the type that a literal is being analyzed against, typechecking the AST it produces and ensuring that hygiene is maintained.

The judgement 
\fbox{$\Delta; \Gamma \vdash e\Rightarrow \tau \leadsto \hat{e}$} 
means that from the type context $\Delta$ and the variable context $\Gamma$ we synthesize the type $\tau$ for $e$. The  expression $e$ possibly containing $\lfloor literal \rfloor$ forms is transformed into the expression $\hat{e}$ without literals.
The judgement 
\fbox{$\Delta; \Gamma \vdash e \Leftarrow \tau \leadsto \hat{e}$} 
means that we check $e$ against the type $\tau$ and the expression $e$ is transformed into the expression $\hat{e}$. 


In $\Delta$ we need to keep track not only of a metadata's type, but we need to carry the actual metadata. This is because in the rule $\textit{T-literal}$ in Figure \ref{fig:statics2} we execute the $parse$ method on $\hat{e}_p$ and for this we need to know what the body of the $parse$ method is.


\subsection{Types and Metavalues}
\todo{object types, case types, metavalues - Ligia}

\todo{perhaps the figures below could be used in this section with the reference to the motivating example (Darya)}


\begin{figure}
\begin{lstlisting}
objtype Parser = 
  def parse(s : TokenStream) : ExpAST
\end{lstlisting}
\caption{Wyvern objtype Parser}
\label{fig:typeParser}
\end{figure}
\subsection{Bidirectional Type System}
Our type-checking is syntax-directed and it requires terms to be fully annotated with types where necessary. A commonly used approach for making the syntax more compact is bidirectional type systems, . They introduce two mutually recursive judgements: one for expressions that have enough information in the context to synthesize a type, and one for expressions for which we know what type to expect, thus only needing to check against that type. Unique types can be determined for synthesis expressions, while analytical expressions have to be verified to have the right types. We chose to use bidirectional type systems in our formalism because they leverage the simplicity of syntax-directed type-checking while not needing to carry much additional type information.

In conventional bidirectional type systems, for constructors of a type one can propagate the type information $\tau$
into the term $e$, which means it should be used in the analysis
judgment $e \Leftarrow \tau$. When constructing a type, we do not have information about it and it is intuitive to use the analysis judgement, which is weaker than the synthesis judgement. On the other hand, destructors generate a result of a smaller type from a component of larger type and can be used for synthesis, propagating type information away from the term as in the synthesis judgement $e \Rightarrow \tau$. Our static semantics rules follow this conventional way of reasoning about constructors and destructors.

\subsection{Procedural Parsing}

The parsing of a new TSL is done by procedural parsing: the metaobject of a type contains a $parser$ field of type $Parser$ that has a method (or `procedure', thus the name `procedural parsing') $parse$. The type $Parser$ is defined in Figure \ref{fig:typeParser}. The user can write an arbitrary $parse$ method, specific for his/her new TSL. Another way of defining the parsing of a newly introduced TSL is by giving the grammar of that TSL, as in Figure \ref{f-htmltype}. In this case, the user will have to give an LALR grammar that contains the productions necessary to parse the new type specific language. This grammar will then be compiled to a $parse$ method written in Wyvern code. The grammar thus becomes a TSL for objects of type $Parser$. 

One can think of elements in quotation marks as being TSLs for abstract syntax trees, such as $":body"$ in Figure \ref{f-htmltype}. The ASTs are written in Wyvern, while their nodes can contain elements written in arbitrary type specific languages, surrounded by quotation marks. The elements in quotations will be regarded as TSLs and translated into Wyvern, the language of the AST. C\# expression trees \cite{Csharp} are created in a similar way: code is represented in a tree-like data structure, where each node is an expression (for example, a method call). When creating a variable \lstinline{num} of type \lstinline{int} one would write

 \lstinline{ParameterExpression numParam = Expression.Parameter(typeof(int), "num")}.

 The variable \lstinline{num} is represented by a String, which would be considered a TSL in Wyvern (Wyvern does not have the built-in type String).

\todo{how Parser works - Ligia}
\subsubsection{Quotations as TSLs}
Related work: C\# expression trees (http://msdn.microsoft.com/en-us/library/bb397951.aspx) -- mention also/instead in the section on quotations as TSLs

\todo{Quotations are TSLs - Ligia}
\subsection{Safety}
\todo{type safety - Cyrus}
\todo{discussion on decidability - Cyrus}
\todo{ambiguity - Cyrus}
\subsection{Grammars as TSLs}
\subsubsection{Ambiguity Checking (related to van wyk's work?) - Cyrus}
